<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
        <meta name="description" content="Hidden Markov Model with Javascript">
        <meta name="keywords" content="Hidden Markov Model, HMM, Baum-Welch algorithm, EM algorithm">
        <meta name="author" content="Mathieu Zaradzki">
        
        
        <title>Importance sampling for Bayesian prediction</title>

        <!-- Bootstrap core CSS -->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
        <!-- Custom styles for this template -->
        <link href="../css/styling.css" rel="stylesheet">
        <link href="../css/favicon.ico" rel="shortcut icon">

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
        </script>
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <!--<p>
        When $a \ne 0$, there are two solutions to \(ax^2 + bx + c = 0\) and they are $$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$
        </p>-->
        <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.3.0/Chart.bundle.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jstat/1.6.2/jstat.js"></script>

        <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>

        <script>
            
            // Let the random generator run for a while
            for (var burn=0; burn<25; burn++)
            {
                Math.random();
            }

            var sigmoid = function (x) {
                return 1. / (1. + Math.exp(-x));
            }

            TrueTheta = [-1.27, -0.40, 0.83, 1.23]; // a random draw of independant N(0,1)

            var Games = []; // Observed data
            Games.push( {player1:1, player2:3, winner:null} );
            Games.push( {player1:3, player2:1, winner:null} );
            Games.push( {player1:0, player2:1, winner:null} );
            Games.push( {player1:1, player2:0, winner:null} );
            Games.push( {player1:1, player2:2, winner:null} );
            //Games.push( {player1:2, player2:1, winner:null} );
            Games.push( {player1:2, player2:3, winner:null} );
            Games.push( {player1:3, player2:2, winner:null} );
            Games.push( {player1:0, player2:2, winner:null} );
            Games.push( {player1:2, player2:0, winner:null} );
            Games.push( {player1:0, player2:3, winner:null} );
            Games.push( {player1:3, player2:0, winner:null} );
            // We will estimate this LAST one :
            var newGame = {player1:2, player2:1, winner:null};

            for (var g=0; g<Games.length; g++) { // Generate face "Observed" games according to made-up probabilities
                var i = Games[g].player1;
                var j = Games[g].player2;
                var P_i_vs_j = sigmoid(TrueTheta[i] - TrueTheta[j]);

                if (Math.random()<P_i_vs_j) {
                    Games[g].winner = i;
                }
                else {
                    Games[g].winner = j;
                }
            }

            // Draw table to observed game results
            google.charts.load('current', {'packages':['table']});
            google.charts.setOnLoadCallback(drawTables);
            function drawTables() {
                drawTable1();
                drawTable2();
                drawTable3();
            }

            function drawTable1() {
                var data = new google.visualization.DataTable();
                data.addColumn('number', 'Game #');
                data.addColumn('number', 'Team 1');
                data.addColumn('number', 'Team 2');
                data.addColumn('number', 'Winner');

                var rows = [];
                for (var g=0; g<Games.length; g++) {
                    rows.push( [g+1, Games[g].player1,  Games[g].player2, Games[g].winner] );
                }
                data.addRows( rows );

                var table = new google.visualization.Table(document.getElementById('score_table_div'));
                table.draw(data, {showRowNumber: false, width: '50%', height: '100%'});
            }

            var Thetas = []; // Thetas = [Theta,...] and sigmoid(Theta[i]-Theta[j]) for each "i versus j" Bernoulli
            var Ws = [];

            var nb_simulations = 1000;
            // Probabilistic Model
            var Likelihood = function (game, strengths) {
                var i = game.player1;
                var j = game.player2;
                var w = game.winner;
                var P_i_vs_j = sigmoid(strengths[i] - strengths[j]);

                var P_game = 0;
                if (w == i) {
                    P_game = P_i_vs_j
                }
                else {
                    P_game = 1 - P_i_vs_j
                }
                return P_game;
            }
            var LogLikelihood = function (game, strengths) {
                return Math.log( Likelihood(game, strengths) );
            }
            var GradLogLikelihood = function (game, strengths) {
                // P = log(sigma(p1-p2))
                // Grad(P) = 1/sigma * sigma' = 1/sigma * sigma(delta) * (1-sigma(delta)) = 1-sigma(delta)
                var grads = [];
                for (var s=0; s<strengths.length; s++) {
                    grads.push(0.);
                }
                var i = game.player1;
                var j = game.player2;
                var w = game.winner;

                if (w == i) {
                    var dlt = strengths[i] - strengths[j];
                    grads[i] = 1-sigmoid(dlt);
                    grads[j] = -grads[i];
                }
                else {
                    var dlt = strengths[j] - strengths[i];
                    grads[j] = 1-sigmoid(dlt);
                    grads[i] = -grads[j];
                }
                return grads;
            }
            // Importance Sampling
            for (var k=0; k<nb_simulations; k++) // Monte Carlo
            {
                // Simulate Theta according to Prior (or, more generally, according to Q)
                var Theta = []; // TO DO
                for (var i=0; i<4; i++) {
                    Theta.push( jStat.normal.sample(0, 1) );
                }

                Thetas.push(Theta);

                var P_observations = 1.;

                for (var g=0; g<Games.length; g++) { // Observed games
                    P_observations *= Likelihood(Games[g], Theta);
                }
                W = P_observations; // more generally W = P(O/theta) * Prior(theta) / Q(theta)
                Ws.push(W);
            }
            var W_sum = 0;
            for (var k=0; k<nb_simulations; k++) // Monte Carlo
            {
                W_sum += Ws[k];
            }

            var I = newGame.player1;
            var J = newGame.player2;
            var Ps = [];
            var Pred_P_I_vs_J = 0;
            for (var k=0; k<nb_simulations; k++) // Monte Carlo
            {
                // Compute the prediction here
                var Theta = Thetas[k];
                var weight = Ws[k] / W_sum;
                var P = sigmoid(Theta[I] - Theta[J]);
                Ps.push( P );
                Pred_P_I_vs_J += P * weight;
            }
            var True_P_I_vs_J = sigmoid(TrueTheta[I] - TrueTheta[J]);

            function drawTable2() {
                var data = new google.visualization.DataTable();
                data.addColumn('number', 'Game #');
                data.addColumn('number', 'Team 1');
                data.addColumn('number', 'Team 2');
                data.addColumn('number', 'True prob. 1 wins');
                data.addColumn('number', 'Pred. prob. 1 wins');

                var rows = [];
                rows.push( [12, newGame.player1,  newGame.player2, True_P_I_vs_J, Pred_P_I_vs_J] );

                data.addRows( rows );

                var table = new google.visualization.Table(document.getElementById('prediction_table_div'));
                table.draw(data, {showRowNumber: false, width: '50%', height: '100%'});
            }

            // MLE for frequentist comparison - Newton ascent version
            function MLE1(start, maxiters) {

                // WARNING : the model is insensitive to a common shift in Theta values
                // Thus maybe we should constrain the SUM of Thetas to be 0 ???

                var ThetaMLE = [];
                if (start === null) {
                    start = TrueTheta;
                }
                for (var s=0; s<start.length; s++) {
                    ThetaMLE.push( start[s] ); // INFO : could also randomly pick one of the Thetas !
                }

                var lr = 0.10; // learning rate

                for (var iter=0; iter<maxiters; iter++) {
                    var Grad = [];
                    for (var s=0; s<ThetaMLE.length; s++) {
                        Grad.push( 0 );
                    }
                    for (var g=0; g<Games.length; g++) {
                        Grad = numeric.add( Grad, GradLogLikelihood(Games[g], ThetaMLE) );
                    }
                    //console.log(Grad);
                    //console.log(numeric.norm2(Grad));
                    ThetaMLE = numeric.add(ThetaMLE, numeric.mul(Grad, lr)); // Maximization !
                    //console.log(ThetaMLE);
                }
                //console.log(TrueTheta);
                console.log(start);
                console.log(ThetaMLE);
                return ThetaMLE;
            }
            // MLE for frequentist comparison - dumb version, pick the best in the MC pool
            function MLE2() {

                var ll_total_max = null;
                var theta_max = null;

                for (var t=0; t<Thetas.length; t++) {
                    LLs = []
                    for (var g=0; g<Games.length; g++) {
                        LLs.push( LogLikelihood(Games[g], Thetas[t]) );
                    }
                    var ll_total = numeric.sum( LLs );
                    if (t == 0) {
                        ll_total_max = ll_total;
                        theta_max = Thetas[t];
                    }
                    if (ll_total_max < ll_total) {
                        ll_total_max = ll_total;
                        theta_max = Thetas[t];
                    }
                }
                console.log(theta_max);
                console.log(ll_total_max);
                return theta_max;
            }
            //
            function drawTable3() {
                var data = new google.visualization.DataTable();
                data.addColumn('number', 'Game #');
                data.addColumn('number', 'Team 1');
                data.addColumn('number', 'Team 2');
                data.addColumn('number', 'True prob. T1 wins');
                data.addColumn('number', 'Pred. prob. T1 wins');

                T = MLE1(MLE2(), 100);
                Freq_P_I_vs_J = sigmoid(T[I] - T[J])

                var rows = [];
                rows.push( [12, newGame.player1,  newGame.player2, True_P_I_vs_J, Freq_P_I_vs_J] );

                data.addRows( rows );

                var table = new google.visualization.Table(document.getElementById('freq_prediction_table_div'));
                table.draw(data, {showRowNumber: false, width: '50%', height: '100%'});
            }
            //
        </script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.17.0/vis.js"></script>
        <link href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.17.0/vis.min.css" rel="stylesheet" />
        
    </head>
    <body>
        <div class="container">
        <div class="row"><div class="col-md-10 col-md-offset-1">
            <div class="row">
                <div class="page-header">
                    <h1>Importance sampling for Bayesian prediction</h1>
                    <h2><small>Mathieu ZARADZKI - 2017</small></h2>
                </div>
                <div>
                    <h4>Application example</h4>

                    Bayesian inference and prediction is particularly suited to deal with cases where there is not a lot of data available to fit the probabilistic model of choice.<br>
                    <br>
                    As an example we can think of a sport season : as each game is played we want to refine our prediction on the future games. As a whole there are not that many games a given team will play (one per week at most) during the season, plus we want to start making forecast from the begining of the season even when only 3 games have been played.<br>
                    <br>
                    Even if 3 games is not enough to fit any model with reasonable confidence we should still make use of this data. But we use it only as a way to refine a "Prior" information that represent expert knowledge. Formally this Prior is given as a probability distribution on likely parameter values for the model.<br>
                    <br>
                </div>
            </div>
            <div class="row">
                <h4>Formal problem statement</h4>

                We have a set of observations comprising of explanatory variables and target variables :<br>
                $\mathcal{O}=\left\{ \left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{N},y_{N}\right)\right\}$
                <br><br>

                Our goal is to make a prediction on the target variable value for a new sample.<br>
                Formally we want to compute the probability :<br>
                $P\left(y^{new}\left|x^{new},\mathcal{\mathcal{O}}\right.\right)$
                <br><br>

                In a Bayesian setting we do not assume a single "best" value for $\theta$ so we integrate over its probability distribution as follow :<br>
                $P\left(y^{new}\left|x^{new},\mathcal{\mathcal{O}}\right.\right)=\int P\left(y^{new}\left|x^{new},\theta\right.\right)P\left(\theta\left|\mathcal{O}\right.\right)d\theta$
                <br><br>
            </div>
            <div class="row">
                <h4>Monte Carlo approximation</h4>

                Using Bayes formula we get :<br>
                $P\left(\theta\left|\mathcal{O}\right.\right)=\frac{P\left(\mathcal{O}\left|\theta\right.\right)Prior\left(\theta\right)}{P\left(\mathcal{O}\right)}$
                <br><br>

                The denominator can expressed as an integral :<br>
                $P\left(\mathcal{O}\right)=\int P\left(\mathcal{O}\left|\theta\right.\right)Prior\left(\theta\right)d\theta$
                <br><br>
                
                As the denominator does not depend on $\theta$ we get :<br>
                $P\left(y^{new}\left|x^{new},\mathcal{\mathcal{O}}\right.\right)=\frac{\int P\left(y^{new}\left|x^{new},\theta\right.\right)P\left(\mathcal{O}\left|\theta\right.\right)Prior\left(\theta\right)d\theta}{\int P\left(\mathcal{O}\left|\theta\right.\right)Prior\left(\theta\right)d\theta}$
                <br><br>

                We can rewrite the 2 integrals as expectations as to approximate them via Monte Carlo simulation :<br>
                $P\left(y^{new}\left|x^{new},\mathcal{\mathcal{O}}\right.\right)=\frac{E_{Prior}\left[P\left(y^{new}\left|x^{new},\theta\right.\right)P\left(\mathcal{O}\left|\theta\right.\right)\right]}{E_{Prior}\left[P\left(\mathcal{O}\left|\theta\right.\right)\right]}$
                <br><br>

                More generally we can sample from an auxialiary distribution $Q$ by setting :<br>
                $W_{Q}\left(\theta\right)=\frac{P\left(\mathcal{O}\left|\theta\right.\right)Prior\left(\theta\right)}{Q\left(\theta\right)}$
                <br><br>

                Then we get :<br>
                $P\left(y^{new}\left|x^{new},\mathcal{\mathcal{O}}\right.\right)=\frac{E_{Q}\left[P\left(y^{new}\left|x^{new},\theta\right.\right)W_{Q}\left(\theta\right)\right]}{E_{Q}\left[W_{Q}\left(\theta\right)\right]}$
                <br><br>
                <b>Remark :</b> Computationaly the MC approach is convenient as it only requires to be able to compute the likelihood of the observed data given a set of model parameters. There is no need to find optimize the set of parameters like in the frequentist MLE approach. The inconvenient is that running MC simulations may be "intensive" but with today computers this is not an issue.
                <br><br>
            </div>
            <div class="row">
                <h4>Example</h4>
                For this experiment lets consider a tennis tournament with 4 players.<br>
                The players are labelled 0,1,2,3 and their actual strength (the model need to infer it from scores) is increasing with their labels.<br>
                Lets assume the players are quite young and not well known so we have identical prior distribution on their strength ; a standard gaussian.<br>
                <br>
                The strengths of each opponent are such that the probability of X winning over Y is :<br>
                $P\left(X>Y\left|strengths\right.\right)=\sigma\left(strength\left(X\right)-strength\left(Y\right)\right)$<br>
                <b>Remark :</b> $\sigma$ denotes the sigmoid function that "squash" values between 0 and 1.<br>
                <br>
                Assuming that the outcome of each game is independant (conditionally on strengths) from the other ones we get the likelihood of the observed games as follow :<br>
                $P\left(\mathcal{O}=Games\left|\theta=strengths\right.\right)=\prod_{g\in Games}P\left(g\left(winner\right)>g\left(loser\right)\left|\theta\right.\right)$
                <br><br>
                <b>Remark :</b> The hypothesis of independant game outcomes is <b>not required</b> to apply the <b>Importance Sampling</b> algorithm but for this quick and simple JavaScript demo it is convenient to simplify the likelihood function. 
                <br><br>
                <h5>Observed data</h5>
                <div id="score_table_div" class="text-center"></div>
                <br>
                <h5>Bayesian Monte-Carlo Prediction Average</h5>
                <div id="prediction_table_div" class="text-center"></div>
                <br>
                <h5>Bayesian Monte-Carlo Prediction Histogram</h5>
                <div style="width:75%;" class="center-block">
                    <canvas id="myChart1"></canvas>
                </div>
                <br>
                <h5>Frequentist (MLE) Prediction (still need code review)</h5>
                <p>If you re-run the page several times you will note it varies more than the Bayesian estimate.</p>
                <div id="freq_prediction_table_div" class="text-center"></div>
                <br>
            </div>
        </div></div>
        </div> <!-- /container -->
        <script>
            var ctx1 = document.getElementById("myChart1").getContext("2d");
            var mindex = [];
            var H1s = [];
            for (var b=0; b<10; b++) {
                mindex.push( (0.05 + b*0.10).toFixed(2) ) ;
                H1s.push( 0 );
            }
            var step = 10;
            for (var k=0; k<nb_simulations; k++) {
                var bucket = Math.floor(Ps[k]*10);
                H1s[bucket] = H1s[bucket] + Ws[k];
            }
            var myChart1 = new Chart(ctx1, {
                type: 'bar',
                data: {
                    labels: mindex,
                    datasets: [
                        {
                            label: 'forecast frequency',
                            data: H1s,
                            backgroundColor: 'rgba(255, 90, 90, 1)',
                            borderColor: 'rgba(255, 90, 90, 0.4)',
                            fill: false,
                        },
                    ],
                },
                options: {
                    // see : http://stackoverflow.com/questions/37204298/chartjs-v2-hide-dataset-labels
                    legend: {
                        display: false
                    },
                    scales: {
                        yAxes: [{
                            ticks: {
                                beginAtZero: true
                            }
                        }]
                    },
                },
            });
        </script>
        <!--<script>
            
        </script>-->
    </body>
</html>